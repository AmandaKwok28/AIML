{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltZ7iG88UKYW"
   },
   "source": [
    "# Mount the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hGcZIFkwUGdc",
    "outputId": "a6222a76-a665-476b-a6fd-06aa3f8e9e51"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54fU90CVUTqX"
   },
   "source": [
    "# Create and Validate a BN\n",
    "Ensure that all species dichtomous key sets actually map to the species we intend them to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjTMQ8P1UR7e",
    "outputId": "f97e97fc-5520-460c-c609-f0e75742ac44"
   },
   "outputs": [],
   "source": [
    "# CONFIGS\n",
    "\n",
    "# network dependencies\n",
    "!pip install pgmpy\n",
    "from pgmpy.models import DiscreteBayesianNetwork\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "\n",
    "# loading dichtomous key data\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJ463liIUyCC"
   },
   "outputs": [],
   "source": [
    "# DIRS\n",
    "ROOT = '/content/drive/MyDrive/Semester Project'\n",
    "KEYS_DIR = os.path.join(ROOT, 'data', 'keys.csv')\n",
    "DICHT_DIR = os.path.join(ROOT, 'data', 'feature_names.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAznGF63U6y5"
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "df = pd.read_csv(KEYS_DIR, encoding=\"latin1\")         # Ana's data on dichotomous keys\n",
    "dicht = json.load(open(DICHT_DIR, \"r\"))               # the feature names (key names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jZNKYhjVIdm"
   },
   "outputs": [],
   "source": [
    "# HELPERS\n",
    "def parse_and_fix(x):\n",
    "    \"\"\"Parse Ana's stored vector reliably and convert:\n",
    "       1 -> 1 (present)\n",
    "       0 -> 0 (absent)\n",
    "       anything else (nan, None, '', etc.) -> 2 (not evaluated)\n",
    "    \"\"\"\n",
    "    # First, safely parse the nested string structure\n",
    "    try:\n",
    "        parsed = ast.literal_eval(x)\n",
    "        parsed = ast.literal_eval(parsed)\n",
    "    except Exception:\n",
    "        # fallback: try more normalization\n",
    "        cleaned = x.replace(\"nan\", \"None\").replace(\"NaN\", \"None\")\n",
    "        parsed = ast.literal_eval(ast.literal_eval(cleaned))\n",
    "\n",
    "    vec = []\n",
    "    for v in parsed:\n",
    "        if v in [1, \"1\", True]:\n",
    "            vec.append(1)\n",
    "        elif v in [0, \"0\", False]:\n",
    "            vec.append(0)\n",
    "        else:\n",
    "            vec.append(2)  # NOT EVALUATED\n",
    "    return np.array(vec, dtype=int)\n",
    "\n",
    "\n",
    "def vector_is_valid(vec):\n",
    "    if vec is None:\n",
    "        return False\n",
    "    if isinstance(vec, float) and np.isnan(vec):\n",
    "        return False\n",
    "    arr = np.array(vec, dtype=float)\n",
    "    return not np.isnan(arr).any()\n",
    "\n",
    "def cpd_to_df(cpd):\n",
    "    species = list(cpd.state_names[\"Species\"])\n",
    "    df = pd.DataFrame(cpd.values,\n",
    "                      index=cpd.state_names[cpd.variable],\n",
    "                      columns=species)\n",
    "    if len(df.index) == 2:\n",
    "        df.index = [\"No\", \"Yes\"]\n",
    "    return df\n",
    "\n",
    "# validate these mappings\n",
    "def predict_species(BN_model, evidence_dict, verbose=True):\n",
    "    \"\"\"\n",
    "    Run BN inference given a feature dictionary (evidence).\n",
    "    Returns (predicted_species, probability, full_distribution).\n",
    "    \"\"\"\n",
    "\n",
    "    # set up inference engine\n",
    "    inference = VariableElimination(BN_model)\n",
    "\n",
    "    # run query\n",
    "    query_result = inference.query(\n",
    "        variables=[\"Species\"],\n",
    "        evidence=evidence_dict\n",
    "    )\n",
    "\n",
    "    # extract names and probs\n",
    "    species_names = query_result.state_names[\"Species\"]\n",
    "    probs = query_result.values\n",
    "\n",
    "    # most probable species\n",
    "    max_idx = probs.argmax()\n",
    "    predicted_species = species_names[max_idx]\n",
    "    predicted_prob = probs[max_idx]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n=== Bayesian Network Prediction ===\")\n",
    "        print(\"Predicted species:\", predicted_species)\n",
    "        print(\"Probability:\", predicted_prob)\n",
    "        print(\"\\nFull distribution:\")\n",
    "        for sp, p in zip(species_names, probs):\n",
    "            print(f\"  {sp}: {p:.4f}\")\n",
    "\n",
    "    return predicted_species, predicted_prob, (species_names, probs)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_posterior(species, probs, top_k=15):\n",
    "    \"\"\"\n",
    "    Plot the posterior distribution from the BN.\n",
    "    Optionally show only the top_k species for clarity.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort species by probability\n",
    "    idx = probs.argsort()[::-1]       # descending\n",
    "    species_sorted = [species[i] for i in idx]\n",
    "    probs_sorted = probs[idx]\n",
    "\n",
    "    # Optionally show only top-k most likely species\n",
    "    species_plot = species_sorted[:top_k]\n",
    "    probs_plot = probs_sorted[:top_k]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(species_plot, probs_plot)\n",
    "    # plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"Posterior Probability\")\n",
    "    plt.title(\"Bayesian Network Species Posterior (Top {} Species)\".format(top_k))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjiVHTt6W2gC"
   },
   "outputs": [],
   "source": [
    "# clean the key vector\n",
    "df[\"Full vector\"] = df[\"Full vector\"].apply(parse_and_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PgfJfcuTTf6"
   },
   "outputs": [],
   "source": [
    "# clean names and drop dupes\n",
    "df[\"Species\"] = df[\"Species\"].astype(str).str.strip()\n",
    "df = df.drop_duplicates(subset=\"Species\", keep=\"first\")\n",
    "df = df[df[\"Full vector\"].apply(vector_is_valid)]\n",
    "\n",
    "# fill out key value pairs : (species, json of feature names)\n",
    "keys = {}\n",
    "\n",
    "for species, row in df.iterrows():\n",
    "    vec = row[\"Full vector\"]\n",
    "\n",
    "    # Length check\n",
    "    if len(vec) != len(dicht):\n",
    "        print(f\"ERROR: Species '{row['Species']}' has vector length {len(vec)} (expected {len(dicht)})\")\n",
    "        continue\n",
    "\n",
    "    # Build feature dictionary\n",
    "    feats = {feat: int(val) for feat, val in zip(dicht, vec)}\n",
    "    keys[row[\"Species\"]] = feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ScnctFWWZcsU",
    "outputId": "f0bad4c5-79dc-485a-c00d-5272e0535640"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# CHECK FOR DUPES\n",
    "def build_equivalence_dict(keys):\n",
    "    \"\"\"\n",
    "    Input:  keys = { species_name : {feature_name: value, ...}, ... }\n",
    "    Output: equivalence = { species_name : [species_with_same_vector], ... }\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Reverse-map feature vectors → list of species with that vector\n",
    "    reverse = defaultdict(list)\n",
    "    for sp, featdict in keys.items():\n",
    "        vec_tuple = tuple(featdict.values())   # must be hashable\n",
    "        reverse[vec_tuple].append(sp)\n",
    "\n",
    "    # Step 2: Build equivalence mapping\n",
    "    equivalence = {}\n",
    "    for vec, species_list in reverse.items():\n",
    "        # all species in this group map to the same species_list\n",
    "        for sp in species_list:\n",
    "            equivalence[sp] = species_list\n",
    "\n",
    "    return equivalence, reverse\n",
    "\n",
    "# === RUN IT ===\n",
    "equivalence, reverse_groups = build_equivalence_dict(keys)\n",
    "\n",
    "# Print duplicate groups nicely\n",
    "print(\"\\n=== Duplicate Feature Groups ===\")\n",
    "for vec, group in reverse_groups.items():\n",
    "    if len(group) > 1:\n",
    "        print(group)\n",
    "\n",
    "# Example lookup\n",
    "print(\"\\nmarshallii group:\", equivalence.get(\"marshallii\"))\n",
    "print(\"gambiae group:\", equivalence.get(\"gambiae\", [\"gambiae\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3tbCoHarmU2",
    "outputId": "fbc24091-4fec-49b4-c018-2010a57ce408"
   },
   "outputs": [],
   "source": [
    "print(equivalence.get(\"ardensis\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O4EXLsBuXfiT",
    "outputId": "a8941a18-1e45-4da3-cf57-63e65b07eb04"
   },
   "outputs": [],
   "source": [
    "# build a network\n",
    "rows = []\n",
    "for species, featdict in keys.items():\n",
    "    row = featdict.copy()\n",
    "    row[\"Species\"] = species\n",
    "    rows.append(row)\n",
    "\n",
    "df_bn = pd.DataFrame(rows)\n",
    "\n",
    "def expand_species(df, n=100):\n",
    "    expanded = []\n",
    "    for _, row in df.iterrows():\n",
    "        for _ in range(n):\n",
    "            expanded.append(row.copy())\n",
    "    return pd.DataFrame(expanded)\n",
    "\n",
    "df_train = expand_species(df_bn, n=100)\n",
    "\n",
    "# build model\n",
    "from pgmpy.models import DiscreteBayesianNetwork\n",
    "features = [c for c in df_train.columns if c != \"Species\"]\n",
    "edges = [(\"Species\", feat) for feat in features]\n",
    "\n",
    "BN_model = DiscreteBayesianNetwork(edges)\n",
    "\n",
    "\n",
    "# fit to cpd\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "BN_model.fit(df_train, estimator=BayesianEstimator, prior_type=\"BDeu\", equivalent_sample_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvATyXRFXk1o",
    "outputId": "43c10b0d-6c39-4b6d-f6c0-c758f233385c"
   },
   "outputs": [],
   "source": [
    "# validate the model with equivalence groups\n",
    "validated = True\n",
    "for species, featdict in keys.items():\n",
    "    evidence = featdict.copy()\n",
    "    pred, _, _ = predict_species(BN_model, evidence, verbose=False)\n",
    "\n",
    "    allowed = equivalence.get(species, [species])\n",
    "    if pred not in allowed:\n",
    "        print(f\"ERROR: Species '{species}' predicted as '{pred}'\")\n",
    "        validated = False\n",
    "\n",
    "if validated:\n",
    "    print(\"\\n=== VALIDATION SUCCESSFUL ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "912twiikbOXZ"
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZ4GiTqTTZ-S",
    "outputId": "e55512cc-b9bf-4b3e-9333-2428f07ad6bd"
   },
   "outputs": [],
   "source": [
    "# load existing dataset from /processed\n",
    "from datasets import load_from_disk\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "DATASET_PATH = \"/content/drive/MyDrive/Semester Project/data-augmentation/processed\"\n",
    "dataset = load_from_disk(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QYbSMh1cTQK"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# import re\n",
    "# if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "#     !pip install unsloth\n",
    "# else:\n",
    "#     # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "#     import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "#     xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "#     !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "#     !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "#     !pip install --no-deps unsloth\n",
    "# !pip install transformers==4.56.2\n",
    "# !pip install --no-deps trl==0.22.2\n",
    "\n",
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512,
     "referenced_widgets": [
      "0f20a0f6012845a1bcf858ef0dea7d38",
      "5e33b0e30c8042d39a3667dbe2f54b03",
      "a3d4c811793246b8a2e76a0e83d426fb",
      "cb1395465f544ccea7f4db51b2d009ec",
      "88b57575045f4b4abc27f3bb574ba067",
      "1dd1650abd2847bb84ff4a514ac35a19",
      "3cfe6d640a844a4887b5ee62781f1735",
      "ebf6f3abdf4349f498c0de109b8e4472",
      "caa26522e2ac4637bce5b5e27790e63c",
      "af0dee7635e9469dbf4584afa3ee879b",
      "c144a086a8714f8f97d77d305bf8ccde",
      "b64df841ae3749bc8007f1da75421120",
      "44dfad915af74860be1653ee6db0cb20",
      "595dcb44119f4a3587388eddd76fd085",
      "c1143b3394354bf9b82b3fc604b5e2fc",
      "f387845e9ff5403cbade6044a9bf9976",
      "a40a3ec2248a47eca56d4495181b3aed",
      "da3023cf6c814fa7a8ec3d18ef90b257",
      "733b88c8888f4fb4a801b05984a35ed4",
      "5cfe12221b1f470bbbda47323bb37240",
      "89a78a2183af4cadb1c8efacd03e0701",
      "77c48f7a02c9424d844c4a0cb3ed2139",
      "33d5cbb8e75343538b42dfdb560fefd9",
      "58a237ab965f4f1f87aad82523dbe0d2",
      "28b7056dd99b4bf58991ec6034597d62",
      "0b6b2a845a2e4db8bbafbbb79924c77d",
      "6545be5749a541d6875583fcb884e54c",
      "fef3edd2d367434882de1585c7b47509",
      "5c3ea72073e64347a0991f7d0e1f9a41",
      "858dc53d456c400792e6c48dd27c2164",
      "57858411d2184a239211b3e956eca4db",
      "62599164187044ad9006480c419bda0e",
      "28b4813905134afb85a3458462bd9051",
      "8a2607c692634fe4bae39fe631fa9145",
      "778604b3f3684824b541bad4a15284cc",
      "42a9935bb48e4842aae20c74904e6c8d",
      "edd0cc3a6a264da2b25318ce2240436a",
      "b033e850631344babfe2ddeda790290d",
      "a7bb0c11c65c4822abd9fba294b32239",
      "9afa6d3c7cec4b078e650dc208006073",
      "3df25be193c143ef975e2103384e092f",
      "243a278f95e74b1d8bd2c12ed418450b",
      "f2e765517697489398279de681119fa6",
      "7b1d7723ed394cdb973f8fb7fb4ace79",
      "25515577d19c4aa88a381f42862b958d",
      "01e08ccc79dc47d7b2a4c532a4934ddf",
      "4fc073d1a6e1421eba369ed2de230fff",
      "ce950ef08b154ff5ac1c5895152460af",
      "b85e8cb36a1f4c97b486b5ebdfdba442",
      "5744d1dcfc3b4720b00816090c5a682f",
      "c1f197f307854bf1a14b30edc3603159",
      "0669758a5970421fb8825fe71a94471d",
      "5e8c58550bd74c28b2cb7b9f6e6d90be",
      "59e0d4f8f3fc4b7da5ae43fa915cb7b8",
      "6da9c18ffcfb4d198e7111f4f994d25c",
      "4ae2f6c37819482f970193af566affac",
      "d644b2a52e984cc8abb6d9a0754610c9",
      "6ebfe4af5db04fd9a26041eef185cdfb",
      "bc95791d6b9c4d58bd35879031116900",
      "ba9f40f27312457fbb21fe2f55f79591",
      "d35e88ae0e714f4d8f97edf0ba50ecff",
      "f6ccc2debb18429c932515bcd79a1ec4",
      "6f65da6e59544d74bd09fa2276f5df26",
      "46e445ef52e2435e95d1f9af4d4e1d99",
      "9547e0bf084e4397888d054a23da7095",
      "071471c680a3418787c27987a3c602ad",
      "de927a578ccb40e5a6fa8d17aa83fb85",
      "d75c2ecb3b804ad89cfd40e7c9132e05",
      "4e8a7f4884fd41a48dcab0ea7560b64b",
      "bcaf4277958744a992da00a3a8ec7f0d",
      "c477603c3ca84595b2d36ed898cf93e4",
      "5df0afd32b284a3cb10db792a0305703",
      "bdbec56a652241f68c9a6015468aeb00",
      "2af1d5b77abc4843907bd5dd0b6271d1",
      "173b35b3633f42eb832b50b00c397141",
      "55f1db280ee04a7bb1a9cbfe1da266f8",
      "cd228dbcb29947639fd52c733bf1ba49",
      "41700ab3656d4cc09b83c74aad38a644",
      "abfe7c8d53f847afb50b523debc0e55e",
      "88a3ed44c43044f484bedd5716e0c4cd",
      "ee20729faba14aa699b434f739d214a6",
      "33c13dd29bca472daca1db8809e50f06",
      "939360148d7d4701a1b72daec464031c",
      "57499ad65b6a4a49b9bb0ff44a24ec9a",
      "bb21e4a70f05492ca4e21868f8cbcba2",
      "4166d7bd2bc345e3822af1e8849cf98c",
      "af1cc86fc18344a6b832739540585a83",
      "ad509b4514294935a6b58704bede2c21",
      "4fac004bf1b747639900a589cc42ab01",
      "eb24c2a2e3ae44f0b3ac7921f0e86a4c",
      "76c1a7b34cbd4c8fa8341d71ce6b47eb",
      "bc083f4981dd4d5ab8264ba2fb27b578",
      "d54a95a347104532af94fa3046836400",
      "4c1a0eaaaacc44eab121700a96de222a",
      "bb4ea2dfbefe4f0f82094dc007284f20",
      "46da9636d3ed4ead87b38c5a8754c97a",
      "8ac316c415a643bb94a711ac4b9012e2",
      "1a6a0b3c3e694caeaf585d90c262f900",
      "4baf8e4628e6498bbfffb11664ca6ad7",
      "0cc732b948314759be5bcf0adbb999b9",
      "8ac9f9a6ee32488eae9251a2f6d09e72",
      "c9ce572d4800482982945fec9005c0f2",
      "ffb3084880ab44128f71fcf01355edcc",
      "4e1879f58707433496b097250630d9f7",
      "2247bd028e6f42e4b78c72281f370248",
      "76d47d738b6d497da7fdb25439270395",
      "0ab10affc9d7490488b7600d2d563a5a",
      "2c1731cbaf8d4fe3b11a5aa5fb016970",
      "fba89832e12f43f2bbad06c213f6b782",
      "b417311bccbb4f4fbdfa9d07047464b8",
      "5431d0e26e904854b2419a5590f88343",
      "4f60fd09cb3d401c84c212acd07edbce",
      "ea26a719cec749429cce4f75ef9e1b2b",
      "af21f46d2b584a4488fb1c87388a276a",
      "841b88bd10e348dfaea83f348a960d81",
      "662ffb0889804dac95457f22e27c0b2c",
      "993f867f76ce459086caf42a82f4055a",
      "790b03ca8c4b43d3aa47e876b334a4cb",
      "405499fb3b6948b7a6539f2f9077f367",
      "6bb65874abe243f98e773af1cf7b7614",
      "345b68af12424745a3e8982af11b2ff0"
     ]
    },
    "id": "XLUtIgI3cNHR",
    "outputId": "b21eda37-97b6-48fd-a272-07bccf8bc4a8"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# 1. Load base model\n",
    "base_model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
    "    load_in_4bit=True,\n",
    "    dtype=torch.float16,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "\n",
    "# 2. Attach your LoRA adapters\n",
    "lora_path = \"/content/drive/MyDrive/Semester Project/data-augmentation/lora_model\"\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gjArBvqTbzq"
   },
   "outputs": [],
   "source": [
    "# load the basic key map (to map machine keys -> dichotomous keys)\n",
    "KEYS_PATH = \"/content/drive/MyDrive/Semester Project/data-augmentation/basic_key_map.json\"\n",
    "\n",
    "with open(KEYS_PATH, \"r\") as f:\n",
    "    basic_key_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dYAQPKsdhER",
    "outputId": "d67e70c5-3025-48d6-da54-e89a03445610"
   },
   "outputs": [],
   "source": [
    "# set up GPT client\n",
    "!pip install openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# set up the api key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"insert your api key here\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjMcaizadCXZ"
   },
   "outputs": [],
   "source": [
    "# post-processing GPT HELPER\n",
    "\n",
    "allowed_keys = basic_key_map.keys()\n",
    "\n",
    "# instruction:\n",
    "def enforce_json_structure(raw_text, allowed_keys):\n",
    "    allowed_keys = list(allowed_keys)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a strict JSON validator.\n",
    "\n",
    "Your ONLY tasks are:\n",
    "1. Ensure the output is valid JSON.\n",
    "2. Ensure the JSON contains exactly this set of keys:\n",
    "{json.dumps(allowed_keys, indent=2)}\n",
    "3. For any key already in the input with value \"present\", \"absent\", or \"occluded\",\n",
    "   DO NOT MODIFY ITS VALUE.\n",
    "4. If a key is missing, add it with value \"occluded\".\n",
    "5. If a key has an invalid value, replace it with \"occluded\".\n",
    "6. Return ONLY a JSON object. No comments, no explanations.\n",
    "\n",
    "Input:\n",
    "{raw_text}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "\n",
    "# decode the json output by mapping the basic keys back to the dichtomous keys\n",
    "# also map present -> 1\n",
    "#          absent  -> 0\n",
    "#          else    -> exclude from json\n",
    "\n",
    "def decode_feature_json(model_json, key_map):\n",
    "    decoded = {}\n",
    "    all_basic_keys = set(key_map.keys())\n",
    "\n",
    "    # 1. Ensure no missing/extra keys\n",
    "    if set(model_json.keys()) != all_basic_keys:\n",
    "        missing = all_basic_keys - set(model_json.keys())\n",
    "        extra   = set(model_json.keys()) - all_basic_keys\n",
    "        raise ValueError(f\"Key mismatch. Missing={missing}, Extra={extra}\")\n",
    "\n",
    "    for basic_key, value in model_json.items():\n",
    "        if basic_key not in key_map:\n",
    "            raise KeyError(f\"Unexpected key '{basic_key}' in model_json\")\n",
    "\n",
    "        v = value.strip().lower()\n",
    "\n",
    "        # 2. Strict validity\n",
    "        if v not in (\"present\", \"absent\", \"occluded\"):\n",
    "            raise ValueError(f\"Invalid value '{value}' for key '{basic_key}'\")\n",
    "\n",
    "        # 3. Map with NO silent fallback\n",
    "        if v == \"present\":\n",
    "            decoded[key_map[basic_key]] = 1\n",
    "        elif v == \"absent\":\n",
    "            decoded[key_map[basic_key]] = 0\n",
    "        else:\n",
    "            decoded[key_map[basic_key]] = 2\n",
    "\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gpmzmrqd8V7"
   },
   "source": [
    "# Perform inference with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axntycfUeDyR"
   },
   "outputs": [],
   "source": [
    "# enable inference mode!\n",
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "# select an image\n",
    "im = 250\n",
    "\n",
    "# select an image to infer from\n",
    "image = dataset[im][\"image\"]\n",
    "\n",
    "# instruction to the VLM\n",
    "instruction = (\n",
    "    \"You are an expert entomologist. Describe accurately what you see in this \"\n",
    "    \"image based on mosquito descriptions present in dichotomous keys.\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9_hmgDPTvwA"
   },
   "outputs": [],
   "source": [
    "# perform inference\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1000,\n",
    "    use_cache=True,\n",
    "    temperature=0.1,\n",
    "    min_p=0.1\n",
    ")\n",
    "\n",
    "# decode into string\n",
    "vlm_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "def clean_vlm_output(raw_output: str) -> str:\n",
    "    # Split on 'assistant' and take everything after it\n",
    "    if \"assistant\" in raw_output:\n",
    "        cleaned = raw_output.split(\"assistant\", 1)[1].strip()\n",
    "    else:\n",
    "        cleaned = raw_output.strip()\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "cleaned_vlm_output = clean_vlm_output(vlm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAIPSECQrM2T",
    "outputId": "bac3d060-2d97-4561-9b5b-fcb706ad1c7e"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(cleaned_vlm_output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCdFFmXsipre"
   },
   "outputs": [],
   "source": [
    "# post process the output\n",
    "model_json = enforce_json_structure(cleaned_vlm_output, list(allowed_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVBVZvQnjDCX"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "if isinstance(model_json, str):\n",
    "    model_json = ast.literal_eval(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfFhCCKQebGX"
   },
   "outputs": [],
   "source": [
    "# post process the output\n",
    "input = decode_feature_json(model_json, basic_key_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "coV2pbcnfvz5",
    "outputId": "650c4fda-405e-4f70-b39a-fc3def645c2a"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(input, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gld3mtc7eliO",
    "outputId": "63959117-87d6-4b53-9f0c-6c1f1a396ec2"
   },
   "outputs": [],
   "source": [
    "# process through the BN\n",
    "pred, _, _ = predict_species(BN_model, input, verbose=True)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8t3eNTeTw_M"
   },
   "outputs": [],
   "source": [
    "# what did the original predict?\n",
    "test = enforce_json_structure(dataset[im][\"caption\"], basic_key_map)\n",
    "test = decode_feature_json(test, basic_key_map)\n",
    "\n",
    "def convert_for_bn(decoded, reverse_map):\n",
    "    out = {}\n",
    "    for full_text, val in decoded.items():\n",
    "        if val in [0, 1] and full_text in reverse_map:\n",
    "            short_key = reverse_map[full_text]\n",
    "            out[short_key] = val\n",
    "    return out\n",
    "\n",
    "test = convert_for_bn(test, basic_key_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2HlIULBvnWhy",
    "outputId": "7988d9a8-c129-4959-98ac-a7f89f14753f"
   },
   "outputs": [],
   "source": [
    "# process through the BN\n",
    "pred2, _, _ = predict_species(BN_model, test, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "qtQDG670LXKO",
    "outputId": "c965e600-eefe-423e-a819-4f1851cf727f"
   },
   "outputs": [],
   "source": [
    "# plot the distrubution\n",
    "pred2, _, posterior = predict_species(BN_model, test, verbose=False)\n",
    "species, probs = posterior\n",
    "plot_posterior(species, probs, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pThcSuyTzCR"
   },
   "outputs": [],
   "source": [
    "# renormalization for fun\n",
    "\n",
    "def restrict_and_renormalize(query_result, allowed_subset):\n",
    "    \"\"\"\n",
    "    Given a pgmpy query_result for variable 'Species',\n",
    "    filter to a subset of species and renormalize probabilities.\n",
    "\n",
    "    Returns:\n",
    "        best_name        – the MAP species inside allowed_subset\n",
    "        best_prob        – its renormalized probability\n",
    "        filtered_names   – list of species kept\n",
    "        filtered_probs   – renormalized probabilities\n",
    "    \"\"\"\n",
    "    # Extract original posterior\n",
    "    species_names = query_result.state_names[\"Species\"]\n",
    "    probs = query_result.values  # array\n",
    "\n",
    "    # Filter\n",
    "    filtered_names = []\n",
    "    filtered_probs = []\n",
    "\n",
    "    for name, prob in zip(species_names, probs):\n",
    "        if name in allowed_subset:\n",
    "            filtered_names.append(name)\n",
    "            filtered_probs.append(prob)\n",
    "\n",
    "    filtered_probs = np.array(filtered_probs)\n",
    "\n",
    "    # Handle edge case: none found\n",
    "    if filtered_probs.sum() == 0:\n",
    "        # Return uniform over subset\n",
    "        uniform_prob = 1.0 / len(filtered_names)\n",
    "        filtered_probs = np.array([uniform_prob] * len(filtered_names))\n",
    "    else:\n",
    "        # Renormalize\n",
    "        filtered_probs /= filtered_probs.sum()\n",
    "\n",
    "    # Pick best\n",
    "    max_idx = filtered_probs.argmax()\n",
    "    best_name = filtered_names[max_idx]\n",
    "    best_prob = filtered_probs[max_idx]\n",
    "\n",
    "    return best_name, best_prob, filtered_names, filtered_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUb-bz6NpryV",
    "outputId": "ca6d0c96-d046-43ce-9f4d-55799f353659"
   },
   "outputs": [],
   "source": [
    "inference = VariableElimination(BN_model)\n",
    "allowed_subset = [\"funestus\", \"gambiae\", \"tenebrosus\", \"pharoensis\", \"coustani\"]\n",
    "\n",
    "query_result = inference.query(\n",
    "    variables=[\"Species\"],\n",
    "    evidence=input\n",
    ")\n",
    "\n",
    "best_name, best_prob, names, probs = restrict_and_renormalize(query_result, allowed_subset)\n",
    "\n",
    "print(\"Most likely species:\", best_name)\n",
    "print(\"Probability:\", best_prob)\n",
    "print(\"Distribution:\")\n",
    "for n, p in zip(names, probs):\n",
    "    print(f\"  {n}: {p:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "YtR_nVqmOjV_",
    "outputId": "24897d76-fde2-4e16-f1e5-50f6a8cffe25"
   },
   "outputs": [],
   "source": [
    "plot_posterior(names, probs, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMCLDPQwN_3t"
   },
   "source": [
    "# Bleu and Rouge Scores\n",
    "note that it doesn't make sense to use these scores because they measure semantic similarity. This is just a demonstration. The actual metric for structured output should be a simple accuracy score for correct key output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWoigui1OCbO"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install evaluate\n",
    "!pip install nltk\n",
    "!pip install rouge_score\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ur07LEv-UTZ7",
    "outputId": "5d740aef-033a-44c3-fb6b-fd1b1db72dbf"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(cleaned_vlm_output, indent=2))\n",
    "print(json.dumps(dataset[im][\"caption\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485,
     "referenced_widgets": [
      "18d87c01f0d4430caa13c754e7e59ff9",
      "0e41fa78882d4e1fa7be55ec3fb16a69",
      "81323fa3922743449c839bfba70a4cff",
      "6d2598ef1a974a97ad483467d6d43320",
      "0da7ccb2dcda49aeba3c2e9525307cf3",
      "323db3e949ec4cb1ac74e445e7bff434",
      "87c46277836c4676bd54bdde5427c892",
      "1b57fe0a6ac84c1592f7092347033ef7",
      "7e24587ad45848868080574613c4096c",
      "94e4e50eb4f84daab3709fdcfacdcd89",
      "e463f41af6de47c386c4504145d9d555",
      "093820d338ef44b7a5d9a41bfa55bc24",
      "dcda9af15c494a978b99ea41f34e95c5",
      "8f6aab6d76d74732888179007eb4fd33",
      "9263824f401346a99b848e83ffbf2a53",
      "c87182b7a5764bd9be13cc92e7bb84ee",
      "40a0742f6c514167ae67c746308ef935",
      "6830da87da554353a185ecfe9918c3d9",
      "b3d8028572464371b0b20643ef3b64b0",
      "127944a3da3346bbbd9f36f83fc180dc",
      "f5416ab8686e4005b51c227227b84dfb",
      "5511cb98eab049feb7c046a36a0d7bad",
      "e9b9854b47394dc2b3dc09a5a442dc56",
      "4cc929aa28e94a468c970bdb3796c603",
      "364ad62687e4459d870148aea9f9728b",
      "5ee84a476cec45f2bdbe8dc9465f18cc",
      "c7b10f5a8ba24885a837734f4ee662e6",
      "b0a1d9c22c3040d49136f6972c36d4d6",
      "08ede68af578435095b01b7ca22f8f0e",
      "1fb797c0282b4cc8ad556e27dadc7593",
      "6027f5d5acd24d9783071b3aef9c2376",
      "1d83162a016b4e049aacf9730b0c0bd9",
      "773a63d7cd374168b79a3a5e9e69e004",
      "687a2a4fa95146b3b3d6dcdc09985e84",
      "fe1ea9e984fa4cfa89de378acf264dab",
      "3cef2052ef75439ba7270185412272c8",
      "068642e6359b4d7086e39205445bc81e",
      "96295239c0134d7f8b27011ce4230a71",
      "844b72ef476c479997ec5d205e8a770e",
      "ca546bc3338942f1a3c9947332817d7b",
      "7c6e44c88b1148159f954d87c17b7ace",
      "46e707cd41944d599b72ffb1a17b1cf6",
      "c965e6a304754a34a71f48174a33c45e",
      "bf3f4e1b61104a16be4c61e179c1ac7a"
     ]
    },
    "id": "Jd1ANxcVOO8u",
    "outputId": "b820301e-b344-436c-e97b-4058e83c81f3"
   },
   "outputs": [],
   "source": [
    "#Compute Metrics\n",
    "# Load the metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# set up the refs\n",
    "ref1 = dataset[im][\"caption\"]\n",
    "\n",
    "# set up the predictions\n",
    "\n",
    "\n",
    "# Example data (can fill it with the dichtomous key stuff later...)\n",
    "predictions = [\n",
    "    json.dumps(input, indent=2),\n",
    "]\n",
    "\n",
    "references = [\n",
    "    json.dumps(ref1, indent=2),\n",
    "]\n",
    "\n",
    "# bleu\n",
    "bleu_result = bleu.compute(\n",
    "    predictions=predictions,\n",
    "    references=references\n",
    ")\n",
    "\n",
    "# rouge\n",
    "rouge_result = rouge.compute(\n",
    "    predictions=predictions,\n",
    "    references=[r[0] for r in references]\n",
    ")\n",
    "\n",
    "print(\"BLEU:\", json.dumps(bleu_result, indent=4))\n",
    "print(\"ROUGE:\", json.dumps(rouge_result, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "509BqHVkVZ_e"
   },
   "source": [
    "# Accuracy of Inference Output Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5Egocz9V9yH",
    "outputId": "eb33ab60-afe4-4873-82d4-b62d9c5af9aa"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(cleaned_vlm_output))\n",
    "print(json.dumps(dataset[im][\"caption\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9eRo11GVb2M",
    "outputId": "fcc0b9b5-d1bf-4590-896b-2043852128ce"
   },
   "outputs": [],
   "source": [
    "# compare model_json vs dataset[im][\"caption\"] keys\n",
    "\n",
    "pred_keys = set(model_json.keys())\n",
    "actual_keys = set(dataset[im][\"caption\"].keys())\n",
    "\n",
    "jaccard = len(actual_keys & pred_keys) / len(actual_keys | pred_keys)\n",
    "print('Jaccard overlap: ', jaccard)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
